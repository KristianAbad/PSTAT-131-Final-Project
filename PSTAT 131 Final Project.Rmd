---
title: "Song Data Project"
author: "Kristian Abad, Steven Truong, Nicole Magallanes"
date: "2/13/2022"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(readr)
library(tidyr)
library(tidyverse)
library(ggplot2)
library(dplyr)
```
# Preprocessing
```{r}
data = read_csv("song_data.csv")
head(data)
```
One challenge we need to figure out is addressing the following cases in our data, if there are any:\

* Remixes
* Remasters
* Single Versions
* Same name but different artists?
* Radio edits
* Extended versions
* Misc mixes/versions
\
I think maybe we can leave remixes possibly treating them as reimaginings of songs or somewhat to the same vein that songs have samples from other tracks are in themselves a separate track. Maybe the more difficult is dealing with the other cases. An example that comes to mind is "Smooth Operator" by Sade (seems like only one of the 3 versions is in the data). There's a single version, a remastered version, and I believe an album version where there's an immediate difference between the remastered and album version.
\
I think the duplicated() function finds exact duplicates of rows.
```{r}
duplicates <- data[duplicated(data),]
duplicates
```
Just testing some cases here...while scrolling through on kaggle, I just picked a random duplicate song to test
```{r}

duplicates %>%
  filter(song_name == 'Zombie')
```
Here's an interesting case where we have 2 of the same rows and 1 with a remix with a track called "8 Letters"
```{r}
duplicates %>%
  filter(song_name == '8 Letters')

duplicates %>%
  filter(song_name == '8 Letters - R3HAB Remix')
```
So it looks like it just picks up exact duplicates and we'll need to figure out what we're going to do with other cases.
```{r}
data_2 <- data[!duplicated(data),]
nrow(data)
nrow(data_2)

nrow(data) - nrow(data_2)
```
Using the grepl function to find any instance of single,remastered, and radio edit/mix versions of tracks. 
```{r}
#Function was found via stackoverflow
#https://stackoverflow.com/questions/10128617/test-if-characters-are-in-a-string
data_3 <- data_2[!(grepl('Single',data_2$song_name,fixed=TRUE) |
                 grepl('Remaster',data_2$song_name,fixed=TRUE) |
                 grepl('Radio',data_2$song_name,fixed=TRUE)),]
data_3
```
Checking for missing values
```{r}
data_3[is.na(data_3)]
```
```{r}
#data_3[grepl('Swimming Pools (Drank)',data_3$song_name,fixed=TRUE),]


#This is the only entry for this song in the dataset
#data_3[grepl("Wouldn't It Be Nice",data_3$song_name,fixed=TRUE),]
#nrow(data_3)
data_4 <- data_3[!(grepl('Mix',data_3$song_name,fixed=TRUE) | 
       grepl('Version',data_3$song_name,fixed=TRUE)),]
data_4
```

# Train/Test Split

```{r}
set.seed(131)
train = sample(1:nrow(data_4), 11240)
music.train = data_4[train,]
music.test = data_4[-train,]

nrow(music.train)
nrow(music.test)
```

# Single decision tree
```{r}
library(tree)
```


```{r}
set.seed(131)
tree.music <- tree(song_popularity ~ . , data_4, subset=train)
summary(tree.music)
```
Plotting the tree
```{r}
plot(tree.music)
text(tree.music, pretty = 0)
```

```{r}
cv.music <- cv.tree(tree.music)
plot(cv.music$size, cv.music$dev, type = "b")
```
```{r}
prune.music <- prune.tree(tree.music, best = 2)
plot(prune.music)
text(prune.music, pretty = 0)
```
```{r}
yhat.tree <- predict(tree.music, newdata = data_4[-train,])
plot(yhat.tree, music.test$song_popularity)
abline(0,1)
mean((yhat.tree - music.test$song_popularity)^2)
```

# Random Forest
\
Here we're going to need the randomForest package.
```{r}
library(randomForest)
```
\
Here we perform bagging :
\
```{r}
set.seed(131)
#Perform bagging
bag.music <- randomForest(song_popularity ~ . , data = data_4, mtry=ncol(data_4) - 1,
                          importance=TRUE,subset=train)
bag.music

```
```{r}
yhat.bag <- predict(bag.music , newdata = data_4[-train,])
music.test.Y <- music.test$song_popularity
plot(yhat.bag, music.test.Y)
abline (0, 1)
mean((yhat.bag - music.test.Y)^2)
```
MSE is pretty high for bagging.\

Let's move on to random forests\
```{r}
set.seed(131)
#Here we create a random forest where we let mtry = p/3 or 14/3
rf.music <- randomForest(song_popularity ~ . , data = data_4,
                          importance=TRUE,subset=train)
rf.music
```

```{r}
yhat.rf <- predict(rf.music , newdata = data_4[-train,])

plot(yhat.rf, music.test.Y)
abline (0, 1)
mean((yhat.rf - music.test.Y)^2)
```
```{r}
set.seed(131)
#Here we create a random forest where we let mtry = 6
rf.music2 <- randomForest(song_popularity ~ . , mtry = 6,data = data_4,
                          importance=TRUE,subset=train)
rf.music2
```

```{r}
yhat.rf2 <- predict(rf.music2 , newdata = data_4[-train,])

plot(yhat.rf2, music.test.Y)
abline (0, 1)
mean((yhat.rf2 - music.test.Y)^2)
```

With the MSE being more or less the same for bagging as it is in random forest, we will proceed to other methods.

```{r}
importance(rf.music)
```
```{r}
varImpPlot(bag.music)
varImpPlot(rf.music)
varImpPlot(rf.music2)
```
Based off these plots we see the MSE increase with certain variables compared to others, namely, energy, instrumentalness, loudness, acousticness, audio_valence, and danceability.

# Boosted trees

```{r}
library(gbm)
```

```{r}
set.seed(131)
boost.music <- gbm(song_popularity ~ . - song_name, data=data_4[train,],
                   distribution = "gaussian", 
                   n.trees=5000,
                   interaction.depth = 4)
```

```{r}
summary(boost.music)
```
We see the most important variables according to the boosted model is loudness, tempo, and song duration in milliseconds. Let's take a look at the partial dependence for these 3 variables:
```{r}
plot(boost.music, i = 'loudness')
plot(boost.music, i = 'tempo') 
plot(boost.music, i = 'song_duration_ms')
```


```{r}
yhat.boost <- predict(boost.music , newdata = data_4[-train,],n.trees = 5000)
plot(yhat.boost, music.test.Y)
abline (0, 1)
mean((yhat.boost - music.test.Y)^2)
```

```{r}
set.seed(131)
boost.music2 <- gbm(song_popularity ~ . - song_name, data=data_4[train,],
                   distribution = "gaussian", 
                   n.trees=5000,
                   interaction.depth = 4,
                   shrinkage = 0.2, verbose = F)
yhat.boost2 <- predict(boost.music2 , newdata = data_4[-train,],n.trees = 5000)
mean((yhat.boost2 - music.test.Y)^2)
```
Hmmm it seems our MSE is increasing with the past 2 models, let's try tuning the shrinkage parameter and see if that helps. Here we'll try $\lambda = 0.001$

```{r}
set.seed(131)
boost.music3 <- gbm(song_popularity ~ . - song_name, data=data_4[train,],
                   distribution = "gaussian", 
                   n.trees=5000,
                   interaction.depth = 4,
                   shrinkage = 0.001, verbose = F)
yhat.boost3 <- predict(boost.music3 , newdata = data_4[-train,],n.trees = 5000)
mean((yhat.boost3 - music.test.Y)^2)
```
We got an improvement for the MSE that relative to the other boosted trees is big but is still high.

```{r}
set.seed(131)
boost.music4 <- gbm(song_popularity ~ . - song_name, data=data_4[train,],
                   distribution = "gaussian", 
                   n.trees=15000,
                   interaction.depth = 4,
                   shrinkage = 0.001,
                   verbose = F)
yhat.boost4 <- predict(boost.music4 , newdata = data_4[-train,],n.trees = 15000)
mean((yhat.boost4 - music.test.Y)^2)
```
We see that the the MSE is slightly getting lower with each addition of 5000 trees. Just for curiosity's sake, let's fit an absurd amount of trees:
```{r}
set.seed(131)
boost.music5 <- gbm(song_popularity ~ . - song_name, data=data_4[train,],
                   distribution = "gaussian", 
                   n.trees=100000,
                   interaction.depth = 4,
                   shrinkage = 0.001,
                   verbose = F)
yhat.boost5 <- predict(boost.music5 , newdata = data_4[-train,],n.trees = 100000)
mean((yhat.boost5 - music.test.Y)^2)
```
```{r}
set.seed(131)
boost.music6 <- gbm(song_popularity ~ . - song_name, data=data_4[train,],
                   distribution = "gaussian", 
                   n.trees=30000,
                   interaction.depth = 4,
                   shrinkage = 0.001,
                   verbose = F)
yhat.boost6 <- predict(boost.music6 , newdata = data_4[-train,],n.trees = 30000)
mean((yhat.boost6 - music.test.Y)^2)
```
```{r}
set.seed(131)
boost.music7 <- gbm(song_popularity ~ . - song_name, data=data_4[train,],
                   distribution = "gaussian", 
                   n.trees=30000,
                   interaction.depth = 6,
                   shrinkage = 0.001,
                   verbose = F)
yhat.boost7 <- predict(boost.music7 , newdata = data_4[-train,],n.trees = 30000)
mean((yhat.boost7 - music.test.Y)^2)
```
```{r}
set.seed(131)
boost.music8 <- gbm(song_popularity ~ . - song_name, data=data_4[train,],
                   distribution = "gaussian", 
                   n.trees=30000,
                   interaction.depth = 8,
                   shrinkage = 0.001,
                   verbose = F)
yhat.boost8 <- predict(boost.music8 , newdata = data_4[-train,],n.trees = 30000)
mean((yhat.boost8 - music.test.Y)^2)
```










